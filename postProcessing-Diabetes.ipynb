{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0f0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS (1 riga per JSON/modello):\n",
      "   layers     ru     encoding hadamard  n_runs  \\\n",
      "0       8   True  Hamiltonian     None      10   \n",
      "1       4  False  Hamiltonian     None      10   \n",
      "2       8  False  Hamiltonian     None      10   \n",
      "3       2   True  Hamiltonian     None      10   \n",
      "4       2  False  Hamiltonian     None      10   \n",
      "\n",
      "                                         source_file  train_acc_mean  \\\n",
      "0  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...        0.785342   \n",
      "1  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...        0.736319   \n",
      "2  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...        0.740879   \n",
      "3  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...        0.721824   \n",
      "4  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...        0.733062   \n",
      "\n",
      "   train_acc_var  train_acc_std  train_precision_mean  ...  test_f1_var  \\\n",
      "0       0.000097       0.009826              0.724980  ...     0.001138   \n",
      "1       0.000113       0.010623              0.661138  ...     0.001745   \n",
      "2       0.000058       0.007618              0.667713  ...     0.001390   \n",
      "3       0.000275       0.016588              0.626221  ...     0.009904   \n",
      "4       0.000115       0.010734              0.662778  ...     0.003356   \n",
      "\n",
      "   test_f1_std                                   source_file_loss  \\\n",
      "0     0.033739  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "1     0.041777  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "2     0.037286  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "3     0.099519  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "4     0.057927  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "\n",
      "   train_loss_mean  train_loss_var  train_loss_std  n_runs_loss  \\\n",
      "0         0.469449        0.000019        0.004315           10   \n",
      "1         0.521478        0.000051        0.007175           10   \n",
      "2         0.516342        0.000026        0.005078           10   \n",
      "3         0.535210        0.000569        0.023843           10   \n",
      "4         0.531645        0.000098        0.009910           10   \n",
      "\n",
      "   test_loss_mean  test_loss_var  test_loss_std  \n",
      "0        0.531585       0.000229       0.015124  \n",
      "1        0.520838       0.000058       0.007605  \n",
      "2        0.510743       0.000044       0.006641  \n",
      "3        0.535703       0.001421       0.037692  \n",
      "4        0.522118       0.000236       0.015376  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "BEST/WORST (per Layers, RU):\n",
      "   layers     ru best_encoding  \\\n",
      "0       2  False   Hamiltonian   \n",
      "1       2   True   Hamiltonian   \n",
      "2       4  False   Hamiltonian   \n",
      "3       4   True   Hamiltonian   \n",
      "4       6  False   Hamiltonian   \n",
      "5       6   True   Hamiltonian   \n",
      "6       8  False   Hamiltonian   \n",
      "7       8   True   Hamiltonian   \n",
      "8      10  False   Hamiltonian   \n",
      "9      10   True   Hamiltonian   \n",
      "\n",
      "                                         best_source  test_f1_mean  \\\n",
      "0  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.492604   \n",
      "1  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.482121   \n",
      "2  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.529055   \n",
      "3  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.556928   \n",
      "4  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.534418   \n",
      "5  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.550310   \n",
      "6  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.519079   \n",
      "7  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.539062   \n",
      "8  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.527677   \n",
      "9  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...      0.530793   \n",
      "\n",
      "  worst_encoding                                       worst_source  \\\n",
      "0    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "1    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "2    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "3    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "4    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "5    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "6    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "7    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "8    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "9    Hamiltonian  VQC-AM-Diabetes-Hamiltonian/model_Encoding_Ham...   \n",
      "\n",
      "   worst_test_f1_mean  \n",
      "0            0.492604  \n",
      "1            0.482121  \n",
      "2            0.529055  \n",
      "3            0.556928  \n",
      "4            0.534418  \n",
      "5            0.550310  \n",
      "6            0.519079  \n",
      "7            0.539062  \n",
      "8            0.527677  \n",
      "9            0.530793  \n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import re, json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ROOT = Path(\".\")\n",
    "FOLDERS = [\"VQC-AM-Diabetes-Hamiltonian\"]  # cambia se serve\n",
    "\n",
    "# Match: model_Encoding_<enc>_numLayers_<L>_Hadamard_<...>_Reuploading_<...>_(accuracy|loss).json\n",
    "FNAME_RE = re.compile(\n",
    "    r\"model_Encoding_(?P<encoding>[A-Za-z0-9\\-]+)\"\n",
    "    r\"_numLayers_(?P<layers>\\d+)\"\n",
    "    r\"(?:_Hadamard_(?P<hadamard>True|False))?\"\n",
    "    r\"_Reuploading_(?P<ru>True|False)\"\n",
    "    r\"_(?P<kind>accuracy|loss)\\.json$\"\n",
    ")\n",
    "\n",
    "def meta_from_filename(fp: Path) -> dict | None:\n",
    "    m = FNAME_RE.search(fp.name)\n",
    "    if not m:\n",
    "        return None\n",
    "    g = m.groupdict()\n",
    "    return {\n",
    "        \"encoding\": g[\"encoding\"],\n",
    "        \"layers\": int(g[\"layers\"]),\n",
    "        \"ru\": (g[\"ru\"] == \"True\"),\n",
    "        \"hadamard\": None if g[\"hadamard\"] is None else (g[\"hadamard\"] == \"True\"),\n",
    "        \"kind\": g[\"kind\"],\n",
    "    }\n",
    "\n",
    "def _safe_div(num: float, den: float, *, zero_division: float = 0.0) -> float:\n",
    "    # stile sklearn: se den==0 ritorna zero_division (tipicamente 0)\n",
    "    return float(num) / float(den) if den else float(zero_division)\n",
    "\n",
    "def metrics_from_counts(tp: int, fp: int, tn: int, fn: int, *, zero_division: float = 0.0) -> dict:\n",
    "    tp, fp, tn, fn = map(int, (tp, fp, tn, fn))\n",
    "\n",
    "    precision = _safe_div(tp, tp + fp, zero_division=zero_division)\n",
    "    recall    = _safe_div(tp, tp + fn, zero_division=zero_division)  # TPR\n",
    "    tnr       = _safe_div(tn, tn + fp, zero_division=zero_division)  # specificity\n",
    "    acc       = _safe_div(tp + tn, tp + tn + fp + fn, zero_division=zero_division)\n",
    "    bal_acc   = (recall + tnr) / 2.0\n",
    "    f1        = _safe_div(2 * precision * recall, precision + recall, zero_division=zero_division)\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def stats_from_accuracy_json(fp: Path, *, zero_division: float = 0.0) -> dict | None:\n",
    "    meta = meta_from_filename(fp)\n",
    "    if meta is None or meta[\"kind\"] != \"accuracy\":\n",
    "        return None\n",
    "\n",
    "    obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    if not isinstance(obj, list) or len(obj) == 0:\n",
    "        return None\n",
    "\n",
    "    per_run_rows = []\n",
    "    for run in obj:\n",
    "        for split in (\"train\", \"test\"):\n",
    "            tp = run.get(f\"tp_{split}\")\n",
    "            fpv = run.get(f\"fp_{split}\")\n",
    "            tn = run.get(f\"tn_{split}\")\n",
    "            fn = run.get(f\"fn_{split}\")\n",
    "            if tp is None or fpv is None or tn is None or fn is None:\n",
    "                continue\n",
    "\n",
    "            m = metrics_from_counts(tp, fpv, tn, fn, zero_division=zero_division)\n",
    "            m[\"split\"] = split\n",
    "            per_run_rows.append(m)\n",
    "\n",
    "    if not per_run_rows:\n",
    "        return None\n",
    "\n",
    "    df = pd.DataFrame(per_run_rows)\n",
    "\n",
    "    out = {\n",
    "        \"layers\": meta[\"layers\"],\n",
    "        \"ru\": meta[\"ru\"],\n",
    "        \"encoding\": meta[\"encoding\"],\n",
    "        \"hadamard\": meta[\"hadamard\"],\n",
    "        \"n_runs\": int(df[df[\"split\"] == \"test\"].shape[0]) if (df[\"split\"] == \"test\").any() else int(df.shape[0]),\n",
    "        \"source_file\": str(fp),\n",
    "    }\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        sub = df[df[\"split\"] == split]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for col in (\"acc\", \"precision\", \"recall\", \"balanced_accuracy\", \"f1\"):\n",
    "            s = sub[col].astype(\"float64\")\n",
    "            out[f\"{split}_{col}_mean\"] = s.mean()\n",
    "            out[f\"{split}_{col}_var\"]  = s.var(ddof=1)\n",
    "            out[f\"{split}_{col}_std\"]  = s.std(ddof=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "def stats_from_loss_json(fp: Path) -> dict | None:\n",
    "    meta = meta_from_filename(fp)\n",
    "    if meta is None or meta[\"kind\"] != \"loss\":\n",
    "        return None\n",
    "\n",
    "    obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    if not isinstance(obj, list) or len(obj) == 0:\n",
    "        return None\n",
    "\n",
    "    out = {\n",
    "        \"layers\": meta[\"layers\"],\n",
    "        \"ru\": meta[\"ru\"],\n",
    "        \"encoding\": meta[\"encoding\"],\n",
    "        \"hadamard\": meta[\"hadamard\"],\n",
    "        \"source_file_loss\": str(fp),\n",
    "    }\n",
    "\n",
    "    for split in (\"train\", \"test\"):\n",
    "        vals = [float(run[split]) for run in obj if isinstance(run, dict) and split in run]\n",
    "        if vals:\n",
    "            s = pd.Series(vals, dtype=\"float64\")\n",
    "            out[f\"{split}_loss_mean\"] = s.mean()\n",
    "            out[f\"{split}_loss_var\"]  = s.var(ddof=1)\n",
    "            out[f\"{split}_loss_std\"]  = s.std(ddof=1)\n",
    "            out[\"n_runs_loss\"] = int(s.shape[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_all_models(repo_root: Path, folders: list[str], *, zero_division: float = 0.0) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    acc_rows, loss_rows = [], []\n",
    "    for folder in folders:\n",
    "        base = repo_root / folder\n",
    "        for fp in base.glob(\"**/*.json\"):\n",
    "            meta = meta_from_filename(fp)\n",
    "            if meta is None:\n",
    "                continue\n",
    "            if meta[\"kind\"] == \"accuracy\":\n",
    "                rec = stats_from_accuracy_json(fp, zero_division=zero_division)\n",
    "                if rec is not None:\n",
    "                    acc_rows.append(rec)\n",
    "            elif meta[\"kind\"] == \"loss\":\n",
    "                rec = stats_from_loss_json(fp)\n",
    "                if rec is not None:\n",
    "                    loss_rows.append(rec)\n",
    "\n",
    "    if not acc_rows:\n",
    "        raise RuntimeError(\"Non ho trovato file *_accuracy.json compatibili (o regex non matcha).\")\n",
    "\n",
    "    df_acc = pd.DataFrame(acc_rows)\n",
    "    df_loss = pd.DataFrame(loss_rows) if loss_rows else pd.DataFrame()\n",
    "\n",
    "    return df_acc, df_loss\n",
    "\n",
    "def merge_acc_loss(df_acc: pd.DataFrame, df_loss: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_loss is None or df_loss.empty:\n",
    "        return df_acc.copy()\n",
    "    keys = [\"encoding\", \"layers\", \"ru\", \"hadamard\"]\n",
    "    return df_acc.merge(df_loss, on=keys, how=\"left\")\n",
    "\n",
    "def best_worst_by_layers_ru(df: pd.DataFrame, *, metric: str = \"test_f1_mean\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Per ogni (layers, ru) prende:\n",
    "    - best: riga con max(metric)\n",
    "    - worst: riga con min(metric)\n",
    "    \"\"\"\n",
    "    need = {\"layers\", \"ru\", metric}\n",
    "    missing = need - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Mancano colonne necessarie: {missing}\")\n",
    "\n",
    "    gcols = [\"layers\", \"ru\"]\n",
    "    rows = []\n",
    "    for (layers, ru), grp in df.groupby(gcols, dropna=False):\n",
    "        grp2 = grp.dropna(subset=[metric])\n",
    "        if grp2.empty:\n",
    "            continue\n",
    "        best = grp2.loc[grp2[metric].idxmax()]\n",
    "        worst = grp2.loc[grp2[metric].idxmin()]\n",
    "\n",
    "        rows.append({\n",
    "            \"layers\": layers,\n",
    "            \"ru\": ru,\n",
    "            \"best_encoding\": best.get(\"encoding\"),\n",
    "            \"best_source\": best.get(\"source_file\"),\n",
    "            metric: float(best[metric]),\n",
    "            \"worst_encoding\": worst.get(\"encoding\"),\n",
    "            \"worst_source\": worst.get(\"source_file\"),\n",
    "            f\"worst_{metric}\": float(worst[metric]),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"layers\", \"ru\"]).reset_index(drop=True)\n",
    "\n",
    "# ==========================\n",
    "# ESECUZIONE\n",
    "# ==========================\n",
    "df_acc, df_loss = load_all_models(REPO_ROOT, FOLDERS, zero_division=0.0)\n",
    "df_models = merge_acc_loss(df_acc, df_loss)\n",
    "\n",
    "table = best_worst_by_layers_ru(df_models, metric=\"test_f1_mean\")  # oppure \"test_acc_mean\"\n",
    "\n",
    "print(\"MODELS (1 riga per JSON/modello):\")\n",
    "print(df_models.head())\n",
    "\n",
    "print(\"\\nBEST/WORST (per Layers, RU):\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32dc69bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['layers', 'ru', 'encoding', 'hadamard', 'n_runs', 'source_file',\n",
       "       'train_acc_mean', 'train_acc_var', 'train_acc_std',\n",
       "       'train_precision_mean', 'train_precision_var', 'train_precision_std',\n",
       "       'train_recall_mean', 'train_recall_var', 'train_recall_std',\n",
       "       'train_balanced_accuracy_mean', 'train_balanced_accuracy_var',\n",
       "       'train_balanced_accuracy_std', 'train_f1_mean', 'train_f1_var',\n",
       "       'train_f1_std', 'test_acc_mean', 'test_acc_var', 'test_acc_std',\n",
       "       'test_precision_mean', 'test_precision_var', 'test_precision_std',\n",
       "       'test_recall_mean', 'test_recall_var', 'test_recall_std',\n",
       "       'test_balanced_accuracy_mean', 'test_balanced_accuracy_var',\n",
       "       'test_balanced_accuracy_std', 'test_f1_mean', 'test_f1_var',\n",
       "       'test_f1_std', 'source_file_loss', 'train_loss_mean', 'train_loss_var',\n",
       "       'train_loss_std', 'n_runs_loss', 'test_loss_mean', 'test_loss_var',\n",
       "       'test_loss_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_models.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94af8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models[[\"layers\", \"ru\", \"encoding\", \"test_recall_mean\", \"test_recall_var\"]].sort_values(by=[\"layers\", \"ru\", \"test_recall_mean\"], ascending=[True, True, False]).to_latex(\"diabetes__recall_hamiltonian.tex\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4026aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for test_precision_mean\n",
    "df_models[[\"layers\", \"ru\", \"encoding\", \"test_precision_mean\", \"test_precision_var\"]].sort_values(by=[\"layers\", \"ru\", \"test_precision_mean\"], ascending=[True, True, False]).to_latex(\"diabetes_precision_hamiltonian.tex\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf5ab601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for balanced_accuracy\n",
    "df_models[[\"layers\", \"ru\", \"encoding\", \"test_balanced_accuracy_mean\", \"test_balanced_accuracy_var\"]].sort_values(by=[\"layers\", \"ru\", \"test_balanced_accuracy_mean\"], ascending=[True, True, False]).to_latex(\"diabetes_balanced_accuracy_hamiltonian.tex\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81c2465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for accuracy\n",
    "df_models[[\"layers\", \"ru\", \"encoding\", \"test_acc_mean\", \"test_acc_var\"]].sort_values(by=[\"layers\", \"ru\", \"test_acc_mean\"], ascending=[True, True, False]).to_latex(\"diabetes_accuracy_hamiltonian.tex\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb1b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it for f1 score\n",
    "df_models[[\"layers\", \"ru\", \"encoding\", \"test_f1_mean\", \"test_f1_var\"]].sort_values(by=[\"layers\", \"ru\", \"test_f1_mean\"], ascending=[True, True, False]).to_latex(\"diabetes_f1_hamiltonian.tex\", index=False, float_format=\"%.4f\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quaptor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
